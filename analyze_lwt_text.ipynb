{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import lwt_functions\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import isodate\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "from pydub import AudioSegment\n",
    "import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Set pandas display settings\n",
    "pd.options.display.max_colwidth = None\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape wikipedia page for LWT episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the tables from the LWT Wikipedia page \n",
    "wiki_url = 'https://en.wikipedia.org/wiki/List_of_Last_Week_Tonight_with_John_Oliver_episodes'\n",
    "all_tables = lwt_functions.scrape_wikipedia_tables(wiki_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to those tables that contain episode information\n",
    "season_tables = [table for table in all_tables if 'wikiepisodetable' in table['class']]\n",
    "\n",
    "# Iterate through seasons and compile episode information\n",
    "episode_number, main_segment_title, air_date, viewers = [], [], [], []\n",
    "for season in season_tables: \n",
    "    for row in season.findAll('tr'):\n",
    "        cells = row.findAll('td')\n",
    "        if len(cells) == 4:\n",
    "            episode_number.append(cells[0].find(text=True))\n",
    "            main_segment_title.append(cells[1].findAll(text=True))\n",
    "            air_date.append(cells[2].find(text=True))\n",
    "            viewers.append(cells[3].find(text=True))\n",
    "\n",
    "# Correcting issues with compiled information\n",
    "air_date = [unicodedata.normalize('NFKC', date) for date in air_date]\n",
    "main_segment_title = [''.join(title).strip() for title in main_segment_title]\n",
    "viewers = [float(v)*1000000 for v in viewers if v != 'TBD']\n",
    "\n",
    "# Create list to track the episode's corresponding season number\n",
    "season = []\n",
    "season_number = 0\n",
    "for episode in episode_number:\n",
    "    if episode == '1':\n",
    "        season_number += 1\n",
    "    season.append(season_number)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Season number did not get grabbed in scrape --> added\n",
    "- Episode numbering restarts each season\n",
    "- Main segment title sometimes includes more than 1 --> combined\n",
    "- Air date had unique codes values representing spaces --> normalized\n",
    "- Per wiki, viewers are in millions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert individual field lists into DataFrame\n",
    "lwt_episodes_wiki = pd.DataFrame([season, episode_number, main_segment_title, air_date, viewers])\n",
    "lwt_episodes_wiki = lwt_episodes_wiki.transpose()\n",
    "lwt_episodes_wiki.columns = ['season','episode','main_segment_title','air_date','viewers']\n",
    "lwt_episodes_wiki['episode_overall'] = lwt_episodes_wiki.index + 1\n",
    "lwt_episodes_wiki['air_date'] = pd.to_datetime(lwt_episodes_wiki['air_date'])\n",
    "lwt_episodes_wiki[lwt_episodes_wiki.main_segment_title != ''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output results to CSV\n",
    "lwt_episodes_wiki.to_csv('data/lwt_episodes_wiki.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get info for individual videos from LWT's YouTube channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key generated on GCP \n",
    "with open('reference/youtube_api_key.txt') as f:\n",
    "    api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through LWT channel's videos\n",
    "lwt_youtube_channel = 'UC3XTzVzaHQEd30rQbuvCtTQ'\n",
    "videos = lwt_functions.fetch_all_youtube_videos(lwt_youtube_channel, api_key)\n",
    "\n",
    "lst = []\n",
    "for video in videos['items']:\n",
    "    video_stats = lwt_functions.get_statistics(video['id']['videoId'], api_key)\n",
    "    results_json = {\n",
    "        'channelTitle':video['snippet']['channelTitle'],\n",
    "        'title':video['snippet']['title'],\n",
    "        'publishedAt':video['snippet']['publishedAt'],\n",
    "        'videoId':video['id']['videoId'],\n",
    "        'duration':video_stats['items'][0]['contentDetails']['duration'],\n",
    "        'viewCount':video_stats['items'][0]['statistics']['viewCount'],\n",
    "        'commentCount':video_stats['items'][0]['statistics']['commentCount'],\n",
    "        'likeCount':video_stats['items'][0]['statistics']['likeCount'],\n",
    "        'dislikeCount':video_stats['items'][0]['statistics']['dislikeCount']\n",
    "    }\n",
    "\n",
    "    lst.append(results_json)\n",
    "    \n",
    "# Convert list to DataFrame and output to avoid re-running\n",
    "lwt_episodes_yt = pd.read_json(json.dumps(lst))\n",
    "print(lwt_episodes_yt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwt_episodes_yt.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the first episode of LWT (Season 1, Epsidoe 1) is missing from YouTube\n",
    "print(main_segments_yt.publishedAt.dt.date.min())\n",
    "print(lwt_episodes_wiki.air_date.dt.date.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export YouTube results to CSV to prevent hitting API quota\n",
    "lwt_episodes_yt.to_csv('data/lwt_episodes_yt.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the DOW episodes originally aired on HBO vs YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether all LWT episodes aired on Sunday\n",
    "lwt_episodes_wiki = pd.read_csv('data/lwt_episodes_wiki.csv', header=0, parse_dates=['air_date'])\n",
    "print(f'Total LWT Episodes (before today): {lwt_episodes_wiki[lwt_episodes_wiki.air_date < datetime.datetime.now() - datetime.timedelta(days=1)].shape[0]}') # ran on Sunday so exlcuded today's episode\n",
    "print(f\"Episodes not on Sunday: {lwt_episodes_wiki[lwt_episodes_wiki['air_date'].dt.day_name() != 'Sunday'].sum().sum():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect when LWT episodes appeared on YT\n",
    "lwt_episodes_yt = pd.read_csv('data/lwt_episodes_yt.csv', header=0, parse_dates=['publishedAt'])\n",
    "lwt_episodes_yt['published_date'] = lwt_episodes_yt['publishedAt'].dt.date\n",
    "lwt_episodes_yt['published_dow'] = lwt_episodes_yt['publishedAt'].dt.day_name()\n",
    "pd.DataFrame(lwt_episodes_yt.published_dow.value_counts(normalize=False, sort=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter YouTube videos to find main segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer additional fields\n",
    "lwt_episodes_yt['main_segment_title'] = lwt_episodes_yt['title'].str.split(':').str[0].str.replace('&#39;',\"'\").str.replace('&quot;','\"')\n",
    "lwt_episodes_yt['duration_in_seconds'] = lwt_episodes_yt['duration'].apply(lambda x: isodate.parse_duration(x).total_seconds())\n",
    "lwt_episodes_yt['duration_time'] = lwt_episodes_yt['duration'].apply(lambda x: str(datetime.timedelta(seconds = isodate.parse_duration(x).total_seconds())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out videos we know aren't main segments based on their title\n",
    "filter_phrases = ['web exclusive','how is this still a thing','official trailer','extended interview','dancing zebra footage','mercadeo ']\n",
    "main_segments_yt = lwt_episodes_yt.copy()\n",
    "for phrase in filter_phrases:\n",
    "    main_segments_yt = main_segments_yt[~main_segments_yt['title'].str.lower().str.contains(phrase)]\n",
    "\n",
    "# Filter out videos less than 5 minutes in duration\n",
    "# Assuming main segments are typically longer\n",
    "main_segments_yt = main_segments_yt[main_segments_yt.duration_in_seconds >= 60*5]\n",
    "\n",
    "# Filter videos that didn't get published to YouTube on Monday\n",
    "main_segments_yt = main_segments_yt[main_segments_yt.published_dow == 'Monday']\n",
    "main_segments_yt.published_dow.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- There doesn't appear to be a YouTube video for LWT season 1 episode 1's main segment\n",
    "- There are only 198 LWT episodes per wiki but 209 videos of \"main stories\" on YouTube\n",
    "- To address this, going to keep only the longest videos\n",
    "- For dates on Youtube with 2+ videos, keep only 1 with longest duration (assuming main segments longer than joke segments)\n",
    "- Assumes we need to eliminate 11 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which dates have 2+ videos\n",
    "# There are 9 dates with 2+ videos published\n",
    "yt_published_date_count = pd.DataFrame(main_segments_yt.groupby(['published_date'])['videoId'].count()).reset_index()\n",
    "yt_published_date_count_two_plus = yt_published_date_count[yt_published_date_count.videoId >= 2]\n",
    "print(yt_published_date_count_two_plus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the videos on those dates with the longest duration \n",
    "duplicate_dates = main_segments_yt[main_segments_yt.published_date.isin(yt_published_date_count_two_plus.published_date)].sort_values(by='published_date')\n",
    "duplicate_dates['duration_rank'] = duplicate_dates.groupby('published_date')['duration_in_seconds'].rank(\"dense\", ascending=False)\n",
    "duplicate_dates = duplicate_dates[duplicate_dates.duration_rank > 1]\n",
    "print(duplicate_dates.shape)\n",
    "\n",
    "# Remove shorter videos  from main segments dataframe\n",
    "main_segments_yt = main_segments_yt[~main_segments_yt.videoId.isin(duplicate_dates.videoId)]\n",
    "main_segments_yt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the Youtube and Wiki datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HYPOTHESIS\n",
    "- New episodes of LWT air on HBO on Sunday nights\n",
    "- Assuming YouTube videos of the main story are published same day or next day\n",
    "- We can use the [near-] matching dates to connect episodes with YouTube video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframes to merge\n",
    "lwt_episodes_wiki = lwt_episodes_wiki.reset_index()\n",
    "lwt_episodes_wiki = lwt_episodes_wiki[lwt_episodes_wiki.air_date < datetime.datetime.now() - datetime.timedelta(days=1)] # running on Sunday so need to exclude today's episode\n",
    "lwt_episodes_wiki['wiki_join_field'] = lwt_episodes_wiki.air_date + pd.DateOffset(1)\n",
    "lwt_episodes_wiki.set_index('wiki_join_field', inplace=True)\n",
    "assert lwt_episodes_wiki.index.duplicated().sum() == 0\n",
    "\n",
    "main_segments_yt = main_segments_yt.reset_index()\n",
    "main_segments_yt.set_index('published_date', inplace=True)\n",
    "main_segments_yt.index = pd.to_datetime(main_segments_yt.index)\n",
    "assert main_segments_yt.index.duplicated().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the wiki to YouTube datasets based on air/published date\n",
    "lwt_episodes = lwt_episodes_wiki.join(main_segments_yt, how='left', on=lwt_episodes_wiki.index, lsuffix='_wiki', rsuffix='_yt')\n",
    "print(lwt_episodes.shape)\n",
    "print(f'Missing # of YouTube videos: {lwt_episodes.videoId.isnull().sum()}') # expected result is 1 since missing first episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push merged data to CSV to prevent re-running cells above\n",
    "lwt_episodes.to_csv('data/lwt_episodes.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert YouTube videos to audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 197\n"
     ]
    }
   ],
   "source": [
    "# Prep lists of urls and corresponding filesnames\n",
    "lwt_episodes = pd.read_csv('data/lwt_episodes.csv', header=0, index_col=0, parse_dates=['air_date','publishedAt'])\n",
    "urls = [f'https://www.youtube.com/watch?v={v}' for v in lwt_episodes.videoId.values if not pd.isna(v)]\n",
    "filenames = [re.sub('[^0-9a-zA-Z ]+', '', t.lower()) for t in lwt_episodes.main_segment_title_wiki.values][1:]\n",
    "print(len(urls),len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.youtube.com/watch?v=dXyO_MC9g3k']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[165:166]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] 0Rnq1NpHdmw: Downloading webpage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: 0Rnq1NpHdmw: YouTube said: Unable to extract video data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED: https://www.youtube.com/watch?v=0Rnq1NpHdmw scientific research and science journalism\n"
     ]
    }
   ],
   "source": [
    "# Download YouTube videos as mp3s\n",
    "# Failed: 68, 165\n",
    "\n",
    "try:\n",
    "    for u, f in zip(urls[68:69], filenames[68:69]):\n",
    "        # pass # adding PASS to prevent accidental re-run\n",
    "        lwt_functions.download_youtube_video_mp3(u, f)\n",
    "except:\n",
    "    print(f'FAILED: {u} {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transcribe videos using AWS Transcribe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm using my own personal account, I wanted to get a sense for how much this step in the project would cost). Given the pricing structure below (from https://aws.amazon.com/transcribe/pricing/), we have 197 episodes, each other no longer than about 20 minutes. Using the  10 and 30 minutes prices to get an estimated range, we're looking at a ballpark price between 47 and 142 bucks.\n",
    "\n",
    "<img src=\"images/aws_transcribe_pricing.PNG\" width=\"480\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n"
     ]
    }
   ],
   "source": [
    "# Get list of audio files\n",
    "audio_files = os.listdir('audio')\n",
    "print(len(audio_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-74ef7532638a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maudio_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'audio/{file}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0ms3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload_fileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'last-week-tonight-audio-for-transcription'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\lastweektonight\\lib\\site-packages\\boto3\\s3\\inject.py\u001b[0m in \u001b[0;36mupload_fileobj\u001b[1;34m(self, Fileobj, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[0;32m    537\u001b[0m             \u001b[0mfileobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m             extra_args=ExtraArgs, subscribers=subscribers)\n\u001b[1;32m--> 539\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\lastweektonight\\lib\\site-packages\\s3transfer\\futures.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcancel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\lastweektonight\\lib\\site-packages\\s3transfer\\futures.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;31m# out of this and propogate the exception.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\lastweektonight\\lib\\site-packages\\s3transfer\\futures.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;31m# possible value integer value, which is on the scale of billions of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;31m# years...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_done_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAXINT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;31m# Once done waiting, raise an exception if present or return the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\lastweektonight\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\lastweektonight\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Upload files to S3 bucket\n",
    "s3 = boto3.client('s3')\n",
    "for file in audio_files:\n",
    "    with open(f'audio/{file}', 'rb') as f:\n",
    "        s3.upload_fileobj(f, 'last-week-tonight-audio-for-transcription', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n"
     ]
    }
   ],
   "source": [
    "# Verify all 197 videos uploaded (minus 2 that failed mp3 download)\n",
    "s3 = boto3.client('s3')\n",
    "keys = []\n",
    "for key in s3.list_objects(Bucket='last-week-tonight-audio-for-transcription')['Contents']:\n",
    "    keys.append(key['Key'])\n",
    "print(len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns out that mp3 is not a supported type so converting files to .wav\n",
    "from pydub.utils import which\n",
    "AudioSegment.converter = which(\"ffmpeg\")\n",
    "\n",
    "for file in audio_files:\n",
    "    sound = AudioSegment.from_file(f\"audio/{file}\")\n",
    "    sound.export(f\"audio/{file.split('.mp3')[0]}.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move old mp3 files to new folder JIC\n",
    "mp3_files = glob.glob('audio/*.mp3', )\n",
    "mp3_files = [f.split('\\\\')[1] for f in mp3_files]\n",
    "\n",
    "if not os.path.exists('audio_mp3'):\n",
    "    os.mkdir('audio_mp3')\n",
    "    \n",
    "for file in mp3_files:\n",
    "    os.rename(f\"audio/{file}\", f\"audio_mp3/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete files from S3 bucket\n",
    "client = boto3.client('s3')\n",
    "for k in keys:\n",
    "    client.delete_object(Bucket='last-week-tonight-audio-for-transcription', Key=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not ready yet...\n",
      "Not ready yet...\n",
      "Not ready yet...\n",
      "Not ready yet...\n",
      "Not ready yet...\n"
     ]
    }
   ],
   "source": [
    "# Pass audio files in S3 to Transcribe\n",
    "transcribe = boto3.client('transcribe', region_name='us-east-1')\n",
    "\n",
    "for i, audio in enumerate(audio_files[:3]):\n",
    "    job_name = f'Transcribe_{i}_{audio.replace(\" \",\"_\")}'\n",
    "    job_uri = f'https://last-week-tonight-audio-for-transcription.s3.amazonaws.com/{audio.replace(\" \",\"+\")}'\n",
    "    transcribe.start_transcription_job(\n",
    "        TranscriptionJobName=job_name,\n",
    "        Media={'MediaFileUri': job_uri},\n",
    "        MediaFormat='mp3',\n",
    "        LanguageCode='en-US'\n",
    "    )\n",
    "    while True:\n",
    "        status = transcribe.get_transcription_job(TranscriptionJobName=job_name)\n",
    "        if status['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:\n",
    "            break\n",
    "        print(\"Not ready yet...\")\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if status['TranscriptionJob']['TranscriptionJobStatus'] == 'COMPLETED':\n",
    "    response = urllib.urlopen(status['TranscriptionJob']['Transcript']['TranscriptFileUri'])\n",
    "    data = json.loads(response.read())\n",
    "    text = data['results']['transcripts'][0]['transcript']\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load transcribed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text files\n",
    "with open('asr/asrOutput.json') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = data['results']['transcripts'][0]['transcript']\n",
    "txt[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lower-case\n",
    "text = txt.lower()\n",
    "\n",
    "# Remove certain items\n",
    "text = text.replace('. ',' ')\n",
    "to_match = [\",\",\"'\",\"\\?\",\"$\",\"%\"] \n",
    "text = re.sub(r'|'.join(to_match), '', text)\n",
    "\n",
    "# Tokenize words\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:8], end='\\n')\n",
    "print(f'Main Story Length: {len(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences that contain a number\n",
    "[n for n in tokens if n.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://online.rapidresizer.com/photograph-to-pattern.php\n",
    "# Plot the most used words before removing stopwords\n",
    "fig = plt.figure(figsize=(20,10), facecolor='lightgrey')\n",
    "fd = nltk.FreqDist(tokens)\n",
    "plt.title('Top 30 Most Used Words')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "fd.plot(30,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most used words after removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [i for i in tokens if not i in stop_words]\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(20,10), facecolor='lightgrey')\n",
    "fd = nltk.FreqDist(tokens)\n",
    "plt.title('Top 30 Most Used Words')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "fd.plot(30,cumulative=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a masked world cloud of John Oliver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove background color\n",
    "# https://burner.bonanza.com/background_burns/42420682/load?panel=replace_background\n",
    "# Change background color (black/transparent) to white\n",
    "# https://www5.lunapic.com/editor/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the mask image\n",
    "john_oliver = np.array(Image.open('jo_white_background.png'))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=500, mask=john_oliver,\n",
    "               stopwords=stopwords, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# generate word cloud\n",
    "wc.generate(txt)\n",
    "\n",
    "# store to file\n",
    "wc.to_file(\"john_oliver.png\")\n",
    "\n",
    "# show\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.imshow(john_oliver, cmap=plt.cm.gray, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "# Read the whole text.\n",
    "# text = open(path.join(d, 'alice.txt')).read()\n",
    "\n",
    "# read the mask image\n",
    "# taken from\n",
    "# http://www.stencilry.org/stencils/movies/alice%20in%20wonderland/255fk.jpg\n",
    "alice_mask = np.array(Image.open(path.join(d, \"jo_white_background.png\")))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"said\")\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=2000, mask=alice_mask,\n",
    "               stopwords=stopwords, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# generate word cloud\n",
    "wc.generate(txt)\n",
    "\n",
    "# store to file\n",
    "wc.to_file(path.join(d, \"alice.png\"))\n",
    "\n",
    "# create coloring from image\n",
    "image_colors = ImageColorGenerator(alice_mask)\n",
    "\n",
    "# show\n",
    "plt.figure(figsize=(20,10))\n",
    "fig, axes = plt.subplots(1, 3, )\n",
    "axes[0].imshow(wc, interpolation=\"bilinear\")\n",
    "# recolor wordcloud and show\n",
    "# we could also give color_func=image_colors directly in the constructor\n",
    "axes[1].imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "axes[2].imshow(alice_mask, cmap=plt.cm.gray, interpolation=\"bilinear\")\n",
    "for ax in axes:\n",
    "    ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the cuss words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cuss word reference file\n",
    "with open('reference/cuss_words.txt') as f:\n",
    "    cuss_words = f.read().strip().split('\\n')\n",
    "    \n",
    "sorted(cuss_words)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of cuss words used\n",
    "word_found = []\n",
    "for word in tokens:\n",
    "    for cuss in cuss_words:\n",
    "        if re.search(cuss, word):\n",
    "            word_found.append(word)\n",
    "print(len(word_found))\n",
    "\n",
    "# Sentences with cuss words\n",
    "sentences = sent_tokenize(txt)\n",
    "sent_found = []\n",
    "for sentence in sentences:\n",
    "    for cuss in cuss_words:\n",
    "        if re.search(cuss, sentence):\n",
    "            sent_found.append(sentence)\n",
    "print(len(set(sent_found)))\n",
    "set(sent_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part 1: Scrape LWT wikipedia page for episodes\n",
    "Part 2: Identify LWT YouTube channel\n",
    "Part 3: Compare dates between wikipedia and YouTube\n",
    "\n",
    "Goals for today:\n",
    "-Create a word cloud using John Oliver image -> DONE\n",
    "-Create a cuss-word counter\n",
    "-Find each cuss word and go +/- n words\n",
    "-Count sentences that include a number (count as a fact)\n",
    "-Assign the episode(s) an order (chronological order)\n",
    "-Look at bigrams and trigrams\n",
    "\n",
    "Bonus\n",
    "-Sentiment analysis\n",
    "-Look at volume level to guage \n",
    "\n",
    "Bonus x2\n",
    "-Get a second LWT episode text\n",
    "-Write code to get text of new LWT episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

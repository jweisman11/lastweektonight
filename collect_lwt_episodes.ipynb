{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import lwt_functions\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import isodate\n",
    "import datetime\n",
    "import json\n",
    "import urllib\n",
    "import re\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "from pydub import AudioSegment\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Set pandas display settings\n",
    "pd.options.display.max_colwidth = None\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape wikipedia page for LWT episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the tables from the LWT Wikipedia page \n",
    "wiki_url = 'https://en.wikipedia.org/wiki/List_of_Last_Week_Tonight_with_John_Oliver_episodes'\n",
    "all_tables = lwt_functions.scrape_wikipedia_tables(wiki_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to those tables that contain episode information\n",
    "season_tables = [table for table in all_tables if 'wikiepisodetable' in table['class']]\n",
    "\n",
    "# Iterate through seasons and compile episode information\n",
    "episode_number, main_segment_title, air_date, viewers = [], [], [], []\n",
    "for season in season_tables: \n",
    "    for row in season.findAll('tr'):\n",
    "        cells = row.findAll('td')\n",
    "        if len(cells) == 4:\n",
    "            episode_number.append(cells[0].find(text=True))\n",
    "            main_segment_title.append(cells[1].findAll(text=True))\n",
    "            air_date.append(cells[2].find(text=True))\n",
    "            viewers.append(cells[3].find(text=True))\n",
    "\n",
    "# Correcting issues with compiled information\n",
    "air_date = [unicodedata.normalize('NFKC', date) for date in air_date]\n",
    "main_segment_title = [''.join(title).strip() for title in main_segment_title]\n",
    "viewers = [float(v)*1000000 for v in viewers if v != 'TBD']\n",
    "\n",
    "# Create list to track the episode's corresponding season number\n",
    "season = []\n",
    "season_number = 0\n",
    "for episode in episode_number:\n",
    "    if episode == '1':\n",
    "        season_number += 1\n",
    "    season.append(season_number)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Season number did not get grabbed in scrape --> added\n",
    "- Episode numbering restarts each season\n",
    "- Main segment title sometimes includes more than 1 --> combined\n",
    "- Air date had unique codes values representing spaces --> normalized\n",
    "- Per wiki, viewers are in millions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert individual field lists into DataFrame\n",
    "lwt_episodes_wiki = pd.DataFrame([season, episode_number, main_segment_title, air_date, viewers])\n",
    "lwt_episodes_wiki = lwt_episodes_wiki.transpose()\n",
    "lwt_episodes_wiki.columns = ['season','episode','main_segment_title','air_date','viewers']\n",
    "lwt_episodes_wiki['episode_overall'] = lwt_episodes_wiki.index + 1\n",
    "lwt_episodes_wiki['air_date'] = pd.to_datetime(lwt_episodes_wiki['air_date'])\n",
    "lwt_episodes_wiki[lwt_episodes_wiki.main_segment_title != ''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output results to CSV\n",
    "lwt_episodes_wiki.to_csv('data/lwt_episodes_wiki.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get info for individual videos from LWT's YouTube channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key generated on GCP \n",
    "with open('reference/youtube_api_key.txt') as f:\n",
    "    api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through LWT channel's videos\n",
    "lwt_youtube_channel = 'UC3XTzVzaHQEd30rQbuvCtTQ'\n",
    "videos = lwt_functions.fetch_all_youtube_videos(lwt_youtube_channel, api_key)\n",
    "\n",
    "lst = []\n",
    "for video in videos['items']:\n",
    "    video_stats = lwt_functions.get_statistics(video['id']['videoId'], api_key)\n",
    "    results_json = {\n",
    "        'channelTitle':video['snippet']['channelTitle'],\n",
    "        'title':video['snippet']['title'],\n",
    "        'publishedAt':video['snippet']['publishedAt'],\n",
    "        'videoId':video['id']['videoId'],\n",
    "        'duration':video_stats['items'][0]['contentDetails']['duration'],\n",
    "        'viewCount':video_stats['items'][0]['statistics']['viewCount'],\n",
    "        'commentCount':video_stats['items'][0]['statistics']['commentCount'],\n",
    "        'likeCount':video_stats['items'][0]['statistics']['likeCount'],\n",
    "        'dislikeCount':video_stats['items'][0]['statistics']['dislikeCount']\n",
    "    }\n",
    "\n",
    "    lst.append(results_json)\n",
    "    \n",
    "# Convert list to DataFrame and output to avoid re-running\n",
    "lwt_episodes_yt = pd.read_json(json.dumps(lst))\n",
    "print(lwt_episodes_yt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwt_episodes_yt.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the first episode of LWT (Season 1, Epsidoe 1) is missing from YouTube\n",
    "print(main_segments_yt.publishedAt.dt.date.min())\n",
    "print(lwt_episodes_wiki.air_date.dt.date.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export YouTube results to CSV to prevent hitting API quota\n",
    "lwt_episodes_yt.to_csv('data/lwt_episodes_yt.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the DOW episodes originally aired on HBO vs YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether all LWT episodes aired on Sunday\n",
    "lwt_episodes_wiki = pd.read_csv('data/lwt_episodes_wiki.csv', header=0, parse_dates=['air_date'])\n",
    "print(f'Total LWT Episodes (before today): {lwt_episodes_wiki[lwt_episodes_wiki.air_date < datetime.datetime.now() - datetime.timedelta(days=1)].shape[0]}') # ran on Sunday so exlcuded today's episode\n",
    "print(f\"Episodes not on Sunday: {lwt_episodes_wiki[lwt_episodes_wiki['air_date'].dt.day_name() != 'Sunday'].sum().sum():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect when LWT episodes appeared on YT\n",
    "lwt_episodes_yt = pd.read_csv('data/lwt_episodes_yt.csv', header=0, parse_dates=['publishedAt'])\n",
    "lwt_episodes_yt['published_date'] = lwt_episodes_yt['publishedAt'].dt.date\n",
    "lwt_episodes_yt['published_dow'] = lwt_episodes_yt['publishedAt'].dt.day_name()\n",
    "pd.DataFrame(lwt_episodes_yt.published_dow.value_counts(normalize=False, sort=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter YouTube videos to find main segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer additional fields\n",
    "lwt_episodes_yt['main_segment_title'] = lwt_episodes_yt['title'].str.split(':').str[0].str.replace('&#39;',\"'\").str.replace('&quot;','\"')\n",
    "lwt_episodes_yt['duration_in_seconds'] = lwt_episodes_yt['duration'].apply(lambda x: isodate.parse_duration(x).total_seconds())\n",
    "lwt_episodes_yt['duration_time'] = lwt_episodes_yt['duration'].apply(lambda x: str(datetime.timedelta(seconds = isodate.parse_duration(x).total_seconds())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out videos we know aren't main segments based on their title\n",
    "filter_phrases = ['web exclusive','how is this still a thing','official trailer','extended interview','dancing zebra footage','mercadeo ']\n",
    "main_segments_yt = lwt_episodes_yt.copy()\n",
    "for phrase in filter_phrases:\n",
    "    main_segments_yt = main_segments_yt[~main_segments_yt['title'].str.lower().str.contains(phrase)]\n",
    "\n",
    "# Filter out videos less than 5 minutes in duration\n",
    "# Assuming main segments are typically longer\n",
    "main_segments_yt = main_segments_yt[main_segments_yt.duration_in_seconds >= 60*5]\n",
    "\n",
    "# Filter videos that didn't get published to YouTube on Monday\n",
    "main_segments_yt = main_segments_yt[main_segments_yt.published_dow == 'Monday']\n",
    "main_segments_yt.published_dow.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- There doesn't appear to be a YouTube video for LWT season 1 episode 1's main segment\n",
    "- There are only 198 LWT episodes per wiki but 209 videos of \"main stories\" on YouTube\n",
    "- To address this, going to keep only the longest videos\n",
    "- For dates on Youtube with 2+ videos, keep only 1 with longest duration (assuming main segments longer than joke segments)\n",
    "- Assumes we need to eliminate 11 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which dates have 2+ videos\n",
    "# There are 9 dates with 2+ videos published\n",
    "yt_published_date_count = pd.DataFrame(main_segments_yt.groupby(['published_date'])['videoId'].count()).reset_index()\n",
    "yt_published_date_count_two_plus = yt_published_date_count[yt_published_date_count.videoId >= 2]\n",
    "print(yt_published_date_count_two_plus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the videos on those dates with the longest duration \n",
    "duplicate_dates = main_segments_yt[main_segments_yt.published_date.isin(yt_published_date_count_two_plus.published_date)].sort_values(by='published_date')\n",
    "duplicate_dates['duration_rank'] = duplicate_dates.groupby('published_date')['duration_in_seconds'].rank(\"dense\", ascending=False)\n",
    "duplicate_dates = duplicate_dates[duplicate_dates.duration_rank > 1]\n",
    "print(duplicate_dates.shape)\n",
    "\n",
    "# Remove shorter videos  from main segments dataframe\n",
    "main_segments_yt = main_segments_yt[~main_segments_yt.videoId.isin(duplicate_dates.videoId)]\n",
    "main_segments_yt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the Youtube and Wiki datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HYPOTHESIS\n",
    "- New episodes of LWT air on HBO on Sunday nights\n",
    "- Assuming YouTube videos of the main story are published same day or next day\n",
    "- We can use the [near-] matching dates to connect episodes with YouTube video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframes to merge\n",
    "lwt_episodes_wiki = lwt_episodes_wiki.reset_index()\n",
    "lwt_episodes_wiki = lwt_episodes_wiki[lwt_episodes_wiki.air_date < datetime.datetime.now() - datetime.timedelta(days=1)] # running on Sunday so need to exclude today's episode\n",
    "lwt_episodes_wiki['wiki_join_field'] = lwt_episodes_wiki.air_date + pd.DateOffset(1)\n",
    "lwt_episodes_wiki.set_index('wiki_join_field', inplace=True)\n",
    "assert lwt_episodes_wiki.index.duplicated().sum() == 0\n",
    "\n",
    "main_segments_yt = main_segments_yt.reset_index()\n",
    "main_segments_yt.set_index('published_date', inplace=True)\n",
    "main_segments_yt.index = pd.to_datetime(main_segments_yt.index)\n",
    "assert main_segments_yt.index.duplicated().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the wiki to YouTube datasets based on air/published date\n",
    "lwt_episodes = lwt_episodes_wiki.join(main_segments_yt, how='left', on=lwt_episodes_wiki.index, lsuffix='_wiki', rsuffix='_yt')\n",
    "print(lwt_episodes.shape)\n",
    "print(f'Missing # of YouTube videos: {lwt_episodes.videoId.isnull().sum()}') # expected result is 1 since missing first episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push merged data to CSV to prevent re-running cells above\n",
    "lwt_episodes.to_csv('data/lwt_episodes.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert YouTube videos to audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 197\n"
     ]
    }
   ],
   "source": [
    "# Prep lists of urls and corresponding filesnames\n",
    "lwt_episodes = pd.read_csv('data/lwt_episodes.csv', header=0, index_col=0, parse_dates=['air_date','publishedAt'])\n",
    "urls = [f'https://www.youtube.com/watch?v={v}' for v in lwt_episodes.videoId.values if not pd.isna(v)]\n",
    "filenames = [re.sub('[^0-9a-zA-Z ]+', '', t.lower()) for t in lwt_episodes.main_segment_title_wiki.values][1:]\n",
    "print(len(urls),len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('scientific research and science journalism', 'boris johnson')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[68], filenames[165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download YouTube videos as mp3s\n",
    "# Failed: 68, 165\n",
    "\n",
    "try:\n",
    "    for u, f in zip(urls[68:69], filenames[68:69]):\n",
    "        # pass # adding PASS to prevent accidental re-run\n",
    "        lwt_functions.download_youtube_video_mp3(u, f)\n",
    "except:\n",
    "    print(f'FAILED: {u} {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transcribe videos using AWS Transcribe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm using my own personal account, I wanted to get a sense for how much this step in the project would cost). Given the pricing structure below (from https://aws.amazon.com/transcribe/pricing/), we have 197 episodes, each other no longer than about 20 minutes. Using the  10 and 30 minutes prices to get an estimated range, we're looking at a ballpark price between 47 and 142 bucks.\n",
    "\n",
    "<img src=\"images/aws_transcribe_pricing.PNG\" width=\"480\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of audio files\n",
    "audio_files = os.listdir('audio')\n",
    "print(len(audio_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files to S3 bucket\n",
    "s3 = boto3.client('s3')\n",
    "for file in audio_files:\n",
    "    with open(f'audio/{file}', 'rb') as f:\n",
    "        s3.upload_fileobj(f, 'last-week-tonight-audio-for-transcription', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all 197 videos uploaded (minus 2 that failed mp3 download)\n",
    "s3 = boto3.client('s3')\n",
    "keys = []\n",
    "for key in s3.list_objects(Bucket='last-week-tonight-audio-for-transcription')['Contents']:\n",
    "    keys.append(key['Key'])\n",
    "print(len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns out that mp3 is not a supported type so converting files to .wav\n",
    "from pydub.utils import which\n",
    "AudioSegment.converter = which(\"ffmpeg\")\n",
    "\n",
    "for file in audio_files:\n",
    "    sound = AudioSegment.from_file(f\"audio/{file}\")\n",
    "    sound.export(f\"audio/{file.split('.mp3')[0]}.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move old mp3 files to new folder JIC\n",
    "mp3_files = glob.glob('audio/*.mp3', )\n",
    "mp3_files = [f.split('\\\\')[1] for f in mp3_files]\n",
    "\n",
    "if not os.path.exists('audio_mp3'):\n",
    "    os.mkdir('audio_mp3')\n",
    "    \n",
    "for file in mp3_files:\n",
    "    os.rename(f\"audio/{file}\", f\"audio_mp3/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete files from S3 bucket\n",
    "client = boto3.client('s3')\n",
    "for k in keys:\n",
    "    client.delete_object(Bucket='last-week-tonight-audio-for-transcription', Key=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass audio files in S3 to Transcribe\n",
    "transcribe = boto3.client('transcribe', region_name='us-east-1')\n",
    "\n",
    "for i, audio in enumerate(audio_files[44:]):\n",
    "    job_name = f'Transcribe_{i}_{audio.replace(\" \",\"_\").replace(\"(\",\"_\").replace(\")\",\"_\").replace(\"&\",\"and\")}'\n",
    "    job_uri = f'https://last-week-tonight-audio-for-transcription.s3.amazonaws.com/{audio.replace(\" \",\"+\")}'\n",
    "    transcribe.start_transcription_job(\n",
    "        TranscriptionJobName=job_name,\n",
    "        Media={'MediaFileUri': job_uri},\n",
    "        MediaFormat='wav',\n",
    "        LanguageCode='en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of transcription jobs\n",
    "# Other statuses-> 'QUEUED'|'IN_PROGRESS'|'FAILED'|\n",
    "client = boto3.client('transcribe', region_name='us-east-1')\n",
    "response = client.list_transcription_jobs(\n",
    "    Status='COMPLETED',\n",
    "    JobNameContains='Transcribe_',\n",
    "    #NextToken='string',\n",
    "    MaxResults=100)\n",
    "\n",
    "response2 = client.list_transcription_jobs(\n",
    "    Status='COMPLETED',\n",
    "    JobNameContains='Transcribe_',\n",
    "    NextToken=response['NextToken'],\n",
    "    MaxResults=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n"
     ]
    }
   ],
   "source": [
    "# Compile job names\n",
    "job_names = []\n",
    "for job in response['TranscriptionJobSummaries']:\n",
    "    job_names.append(job['TranscriptionJobName'])\n",
    "for job in response2['TranscriptionJobSummaries']:\n",
    "    job_names.append(job['TranscriptionJobName'])\n",
    "print(len(job_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in job_names:\n",
    "    status = client.get_transcription_job(TranscriptionJobName=job)\n",
    "    if status['TranscriptionJob']['TranscriptionJobStatus'] == 'COMPLETED':\n",
    "        response = urllib.request.urlopen(status['TranscriptionJob']['Transcript']['TranscriptFileUri'])\n",
    "        data = json.loads(response.read())\n",
    "        with open(f\"transcription/{job[:-4]}.json\", 'w') as fp:\n",
    "            json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete transcription jobs\n",
    "transcribe = boto3.client('transcribe', region_name='us-east-1')\n",
    "for job in job_names:\n",
    "    client.delete_transcription_job(TranscriptionJobName=job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
